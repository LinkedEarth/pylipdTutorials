{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d833ae4b",
   "metadata": {},
   "source": [
    "# Reading LiPD formatted datasets with `PyLiPD`\n",
    "\n",
    "## Authors\n",
    "\n",
    "[Deborah Khider](https://orcid.org/0000-0001-7501-8430)\n",
    "\n",
    "\n",
    "## Preamble\n",
    "\n",
    "`PyLiPD` is a Python package that allows you to read, manipulate, and write [LiPD](https://doi.org/10.5194/cp-12-1093-2016) formatted datasets.\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Open LiPD formatted datasets from:\n",
    "    * a local copy\n",
    "    * a web URL\n",
    "    * Our [LipdGraph database](https://linkedearth.graphdb.mint.isi.edu).\n",
    "* Load in parallel\n",
    "* Add more LiPD datasets to an existing object\n",
    "* Merge two `LiPD` objects together\n",
    "\n",
    "Reading Time: 5 minutes\n",
    "\n",
    "### Keywords\n",
    "\n",
    "LiPD\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "None. This tutorial assumes basic knowledge of Python. If you are not familiar with this coding language, check out this tutorial: http://linked.earth/ec_workshops_py/.\n",
    "\n",
    "### Relevant Packages\n",
    "\n",
    "pylipd\n",
    "\n",
    "## Data Description\n",
    "\n",
    "This notebook uses the following datasets, in LiPD format:\n",
    "\n",
    "* Nurhati, I. S., Cobb, K. M., & Di Lorenzo, E. (2011). Decadal-scale SST and salinity variations in the central tropical Pacific: Signatures of natural and anthropogenic climate change. Journal of Climate, 24(13), 3294–3308. doi:10.1175/2011jcli3852.1\n",
    "\n",
    "* Moses, C. S., Swart, P. K., and Rosenheim, B. E. (2006), Evidence of multidecadal salinity variability in the eastern tropical North Atlantic, Paleoceanography, 21, PA3010, doi:10.1029/2005PA001257.\n",
    "\n",
    "* PAGES2k Consortium., Emile-Geay, J., McKay, N. et al. A global multiproxy database for temperature reconstructions of the Common Era. Sci Data 4, 170088 (2017). doi:10.1038/sdata.2017.88\n",
    "\n",
    "* Stott, L., Timmermann, A., & Thunell, R. (2007). Southern Hemisphere and deep-sea warming led deglacial atmospheric CO2 rise and tropical warming. Science (New York, N.Y.), 318(5849), 435–438. doi:10.1126/science.1143791\n",
    "\n",
    "* Tudhope, A. W., Chilcott, C. P., McCulloch, M. T., Cook, E. R., Chappell, J., Ellam, R. M., et al. (2001). Variability in the El Niño-Southern Oscillation through a glacial-interglacial cycle. Science, 291(1511), 1511-1517. doi:doi:10.1126/science.1057969\n",
    "\n",
    "* Tierney, J. E., Abram, N. J., Anchukaitis, K. J., Evans, M. N., Giry, C., Kilbourne, K. H., et al. (2015). Tropical sea surface temperatures for the past four centuries reconstructed from coral archives. Paleoceanography, 30(3), 226–252. doi:10.1002/2014pa002717\n",
    "\n",
    "* Orsi, A. J., Cornuelle, B. D., and Severinghaus, J. P. (2012), Little Ice Age cold interval in West Antarctica: Evidence from borehole temperature at the West Antarctic Ice Sheet (WAIS) Divide, Geophys. Res. Lett., 39, L09710, doi:10.1029/2012GL051260."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0be91b",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "`PyLiPD` uses object-oriented programming (OOP). In OOP, object contains the data, associated parameters (e.g., metadata) for the object and code that represents procedures that are applicable to each object. OOP is ubiquituous in Python and presents several advantages over method-oriented programming: it follows the natural relationship between an object and a method, with each call representing a clearly defined action.\n",
    "\n",
    "In `PyLiPD` you will only be dealing with the `LiPD` object, so you can import it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021f5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylipd.lipd import LiPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20186ddd",
   "metadata": {},
   "source": [
    "### Loading LiPD formatted datasets from a local file\n",
    "\n",
    "First let's create an empty object, in which we can load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0485ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = LiPD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78601f1",
   "metadata": {},
   "source": [
    "Now let's load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100a98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 46.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/Ocn-Palmyra.Nurhati.2011.lpd'\n",
    "D.load(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82363c",
   "metadata": {},
   "source": [
    "If you want to see the dataset names contained in your object easily, you can use [this function](https://pylipd.readthedocs.io/en/latest/source/pylipd.html#pylipd.lipd.LiPD.get_all_dataset_names), which returns a list of dataset names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941de4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ocn-Palmyra.Nurhati.2011']\n"
     ]
    }
   ],
   "source": [
    "names = D.get_all_dataset_names()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab1f6a",
   "metadata": {},
   "source": [
    "### Loading a LiPD formatted datasets from a url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b5fe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_url = 'https://lipdverse.org/data/iso2k100_CO06MOPE/1_0_2//CO06MOPE.lpd'\n",
    "\n",
    "D2=LiPD()\n",
    "D2.load(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25eb9c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CO06MOPE']\n"
     ]
    }
   ],
   "source": [
    "names = D2.get_all_dataset_names()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b2ed3",
   "metadata": {},
   "source": [
    "If you want to work with both files together, you can simply load the new dataset into your existing object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea5da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "['Ocn-Palmyra.Nurhati.2011', 'CO06MOPE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D.load(data_url)\n",
    "\n",
    "names = D.get_all_dataset_names()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15fb21",
   "metadata": {},
   "source": [
    "You can also create the object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cf44b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "['Ocn-Palmyra.Nurhati.2011', 'CO06MOPE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = ['../data/Ocn-Palmyra.Nurhati.2011.lpd', 'https://lipdverse.org/data/iso2k100_CO06MOPE/1_0_2//CO06MOPE.lpd']\n",
    "\n",
    "D3=LiPD()\n",
    "D3.load(data)\n",
    "\n",
    "names = D3.get_all_dataset_names()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116690cb",
   "metadata": {},
   "source": [
    "### Loading from a directory\n",
    "\n",
    "\n",
    "Let's load some of the datasets contained in the Euro2k database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4504f154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 16 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 94.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = '../data/Pages2k/'\n",
    "\n",
    "D_dir = LiPD()\n",
    "D_dir.load_from_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ea6d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ocn-RedSea.Felis.2000', 'Ant-WAIS-Divide.Severinghaus.2012', 'Asi-SourthAndMiddleUrals.Demezhko.2007', 'Ocn-AlboranSea436B.Nieto-Moreno.2013', 'Eur-SpannagelCave.Mangini.2005', 'Ocn-FeniDrift.Richter.2009', 'Eur-LakeSilvaplana.Trachsel.2010', 'Ocn-PedradeLume-CapeVerdeIslands.Moses.2006', 'Ocn-SinaiPeninsula_RedSea.Moustafa.2000', 'Eur-NorthernSpain.Martin-Chivelet.2011', 'Arc-Kongressvatnet.D_Andrea.2012', 'Eur-CoastofPortugal.Abrantes.2011', 'Eur-SpanishPyrenees.Dorado-Linan.2012', 'Eur-FinnishLakelands.Helama.2014', 'Eur-NorthernScandinavia.Esper.2012', 'Eur-Stockholm.Leijonhufvud.2009']\n"
     ]
    }
   ],
   "source": [
    "names = D_dir.get_all_dataset_names()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904ad70",
   "metadata": {},
   "source": [
    "You can still load single files using the method described above and append them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e022b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "['Ocn-RedSea.Felis.2000', 'Ant-WAIS-Divide.Severinghaus.2012', 'Asi-SourthAndMiddleUrals.Demezhko.2007', 'Ocn-AlboranSea436B.Nieto-Moreno.2013', 'Eur-SpannagelCave.Mangini.2005', 'Ocn-FeniDrift.Richter.2009', 'Eur-LakeSilvaplana.Trachsel.2010', 'Ocn-PedradeLume-CapeVerdeIslands.Moses.2006', 'Ocn-SinaiPeninsula_RedSea.Moustafa.2000', 'Eur-NorthernSpain.Martin-Chivelet.2011', 'Arc-Kongressvatnet.D_Andrea.2012', 'Eur-CoastofPortugal.Abrantes.2011', 'Eur-SpanishPyrenees.Dorado-Linan.2012', 'Eur-FinnishLakelands.Helama.2014', 'Eur-NorthernScandinavia.Esper.2012', 'Eur-Stockholm.Leijonhufvud.2009', 'CO06MOPE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D_dir.load(data_url)\n",
    "\n",
    "names = D_dir.get_all_dataset_names()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f04df",
   "metadata": {},
   "source": [
    "### Loading from the remote LipdGraph database\n",
    "\n",
    "Files stored on the [LiPDverse](https://lipdverse.org) are also available in a [graph database](https://linkedearth.graphdb.mint.isi.edu), which supports complex querying through the [SPARQL](https://en.wikipedia.org/wiki/SPARQL) query language. `PyLiPD` essentially wraps these complex queries into Python calls to facilitate the manipulation of the datasets. \n",
    "\n",
    "To load a file from the remote database, all you need to know is the dataset name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b109580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching datasets from remote endpoint..\n",
      "Making remote query to endpoint: https://linkedearth.graphdb.mint.isi.edu/repositories/LiPDVerse-dynamic\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 401: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lipd_remote \u001b[38;5;241m=\u001b[39m LiPD()\n\u001b[1;32m      2\u001b[0m lipd_remote\u001b[38;5;241m.\u001b[39mset_endpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://linkedearth.graphdb.mint.isi.edu/repositories/LiPDVerse-dynamic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m lipd_remote\u001b[38;5;241m.\u001b[39mload_remote_datasets([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOcn-MadangLagoonPapuaNewGuinea.Kuhnert.2001\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMD98_2181.Stott.2007\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnt-WAIS-Divide.Severinghaus.2012\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(lipd_remote\u001b[38;5;241m.\u001b[39mget_all_dataset_names())\n",
      "File \u001b[0;32m~/Documents/GitHub/pylipd/pylipd/lipd.py:219\u001b[0m, in \u001b[0;36mLiPD.load_remote_datasets\u001b[0;34m(self, dsnames)\u001b[0m\n\u001b[1;32m    217\u001b[0m dsnamestr \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m NSURL \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dsname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dsname \u001b[38;5;129;01min\u001b[39;00m dsnames))\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaching datasets from remote endpoint..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m qres, qres_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT ?s ?p ?o ?g WHERE \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m GRAPH ?g \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m ?s ?p ?o \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m VALUES ?g \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdsnamestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, remote\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Reinitialize graph\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# self._initialize_graph()\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m qres:\n",
      "File \u001b[0;32m~/Documents/GitHub/pylipd/pylipd/utils/rdf_graph.py:185\u001b[0m, in \u001b[0;36mRDFGraph.query\u001b[0;34m(self, query, remote, result)\u001b[0m\n\u001b[1;32m    182\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m SELECT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mvars\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m WHERE \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m SERVICE <\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m> \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m   \n\u001b[1;32m    184\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mquery(query)\n\u001b[0;32m--> 185\u001b[0m result_df \u001b[38;5;241m=\u001b[39m sparql_results_to_df(result)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, result_df\n",
      "File \u001b[0;32m~/Documents/GitHub/pylipd/pylipd/utils/utils.py:92\u001b[0m, in \u001b[0;36msparql_results_to_df\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msparql_results_to_df\u001b[39m(results: SPARQLResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Export results from an rdflib SPARQL query into a `pandas.DataFrame`,\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    using Python types. See https://github.com/RDFLib/rdflib/issues/1179.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\n\u001b[1;32m     93\u001b[0m         data\u001b[38;5;241m=\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtoPython() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m results),\n\u001b[1;32m     94\u001b[0m         columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvars],\n\u001b[1;32m     95\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/site-packages/pandas/core/frame.py:798\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    796\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/Documents/GitHub/pylipd/pylipd/utils/utils.py:93\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msparql_results_to_df\u001b[39m(results: SPARQLResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Export results from an rdflib SPARQL query into a `pandas.DataFrame`,\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    using Python types. See https://github.com/RDFLib/rdflib/issues/1179.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\n\u001b[0;32m---> 93\u001b[0m         data\u001b[38;5;241m=\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtoPython() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m row] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m results),\n\u001b[1;32m     94\u001b[0m         columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvars],\n\u001b[1;32m     95\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/site-packages/rdflib/query.py:373\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# this iterates over ResultRows of variable bindings\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_genbindings:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_genbindings:\n\u001b[1;32m    374\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m b:  \u001b[38;5;66;03m# don't add a result row in case of empty binding {}\u001b[39;00m\n\u001b[1;32m    375\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bindings\u001b[38;5;241m.\u001b[39mappend(b)\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/site-packages/rdflib/plugins/sparql/evaluate.py:562\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevalProject\u001b[39m(ctx: QueryContext, project: CompValue):\n\u001b[1;32m    561\u001b[0m     res \u001b[38;5;241m=\u001b[39m evalPart(ctx, project\u001b[38;5;241m.\u001b[39mp)\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (row\u001b[38;5;241m.\u001b[39mproject(project\u001b[38;5;241m.\u001b[39mPV) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m res)\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/site-packages/rdflib/plugins/sparql/evaluate.py:359\u001b[0m, in \u001b[0;36mevalServiceQuery\u001b[0;34m(ctx, part)\u001b[0m\n\u001b[1;32m    355\u001b[0m     response \u001b[38;5;241m=\u001b[39m urlopen(\n\u001b[1;32m    356\u001b[0m         Request(service_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(query_settings), headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     response \u001b[38;5;241m=\u001b[39m urlopen(\n\u001b[1;32m    360\u001b[0m         Request(\n\u001b[1;32m    361\u001b[0m             service_url,\n\u001b[1;32m    362\u001b[0m             data\u001b[38;5;241m=\u001b[39murlencode(query_settings)\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    363\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m     )\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    367\u001b[0m     json \u001b[38;5;241m=\u001b[39m j\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/paleodev/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 401: "
     ]
    }
   ],
   "source": [
    "lipd_remote = LiPD()\n",
    "lipd_remote.set_endpoint(\"https://linkedearth.graphdb.mint.isi.edu/repositories/LiPDVerse-dynamic\")\n",
    "lipd_remote.load_remote_datasets([\"Ocn-MadangLagoonPapuaNewGuinea.Kuhnert.2001\", \"MD98_2181.Stott.2007\", \"Ant-WAIS-Divide.Severinghaus.2012\"])\n",
    "\n",
    "print(lipd_remote.get_all_dataset_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe668845",
   "metadata": {},
   "source": [
    "### Loading in parallel\n",
    "\n",
    "If you plan on loading mulitple LiPD files (hundreds to thousands), you may want to do so in parallel. If you choose to do so, you need to use the `if __name__ == \"__main__\" ` notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe2d691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 16 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 23.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    D_parallel = LiPD()\n",
    "    D_parallel.load_from_dir(path, parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec99b1",
   "metadata": {},
   "source": [
    "After the intial loading, you can resume using your object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ddfb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eur-SpannagelCave.Mangini.2005', 'Ocn-AlboranSea436B.Nieto-Moreno.2013', 'Asi-SourthAndMiddleUrals.Demezhko.2007', 'Eur-SpanishPyrenees.Dorado-Linan.2012', 'Ocn-SinaiPeninsula_RedSea.Moustafa.2000', 'Eur-NorthernSpain.Martin-Chivelet.2011', 'Ocn-RedSea.Felis.2000', 'Eur-FinnishLakelands.Helama.2014', 'Ocn-PedradeLume-CapeVerdeIslands.Moses.2006', 'Arc-Kongressvatnet.D_Andrea.2012', 'Eur-NorthernScandinavia.Esper.2012', 'Eur-LakeSilvaplana.Trachsel.2010', 'Ocn-FeniDrift.Richter.2009', 'Eur-CoastofPortugal.Abrantes.2011', 'Ant-WAIS-Divide.Severinghaus.2012', 'Eur-Stockholm.Leijonhufvud.2009']\n"
     ]
    }
   ],
   "source": [
    "print(D_parallel.get_all_dataset_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04753448",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Note: </b> Once datasets are loaded into the object with one of the methods described above, you can always append more using a different method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1909ad3",
   "metadata": {},
   "source": [
    "### Merging LiPD objects\n",
    "\n",
    "In the course of your work, you may need to merge two `LiPD` objects together. Let's merge `D` into `D_parallel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3491622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eur-SpannagelCave.Mangini.2005', 'Ocn-AlboranSea436B.Nieto-Moreno.2013', 'Asi-SourthAndMiddleUrals.Demezhko.2007', 'Eur-SpanishPyrenees.Dorado-Linan.2012', 'Ocn-SinaiPeninsula_RedSea.Moustafa.2000', 'Eur-NorthernSpain.Martin-Chivelet.2011', 'Ocn-RedSea.Felis.2000', 'Eur-FinnishLakelands.Helama.2014', 'Ocn-PedradeLume-CapeVerdeIslands.Moses.2006', 'Arc-Kongressvatnet.D_Andrea.2012', 'Eur-NorthernScandinavia.Esper.2012', 'Eur-LakeSilvaplana.Trachsel.2010', 'Ocn-FeniDrift.Richter.2009', 'Eur-CoastofPortugal.Abrantes.2011', 'Ant-WAIS-Divide.Severinghaus.2012', 'Eur-Stockholm.Leijonhufvud.2009', 'Ocn-Palmyra.Nurhati.2011', 'CO06MOPE']\n"
     ]
    }
   ],
   "source": [
    "D_merged = D_parallel.merge(D)\n",
    "\n",
    "print(D_merged.get_all_dataset_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e97333-e3f1-45c9-9127-5aa107a00b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
