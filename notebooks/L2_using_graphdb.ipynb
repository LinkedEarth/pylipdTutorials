{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27eab70d-5356-470c-aea5-3aa58d8419fe",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/LinkedEarth/Logos/blob/master/pyLiPD_logo1_transparent.png?raw=true\">\n",
    "\n",
    "# Directly querying the LiPDGraph\n",
    "\n",
    "## Authors\n",
    "\n",
    "[Deborah Khider](https://orcid.org/0000-0001-7501-8430)\n",
    "\n",
    "## Preamble\n",
    "\n",
    "This tutorial demonstrates a query on the `LiPDGraph` using the direct SPARQL endpoint. You might ask yourself: \"What is the `LiPDGraph` and how does it differ from the Lipdverse?\" The short answer os the the `LiPDGraph` contains all the datasets that are available through Lipdverse, stored in a graph format. Remember tha PyLiPD essentially takes a LiPD file and transforms it into a graph. We have already done this process for the LiPDverse, and the datasets can already be queried there. \n",
    "\n",
    "### Goals\n",
    "\n",
    "* Learn how to query the Graph database directly\n",
    "\n",
    "Reading Time: 5 minutes\n",
    "\n",
    "### Keywords\n",
    "\n",
    "SPARQL\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "* [Understand the basics of RDF and SPAQRL](http://linked.earth/pylipdTutorials/graph.html)\n",
    "* [Querying with SPARQL](L2_custom_queries.md)\n",
    "                                                                               \n",
    "                                                                                \n",
    "### Relevant Packages\n",
    "\n",
    "pandas\n",
    "\n",
    "## Data Description\n",
    "\n",
    "This notebook makes use of the `LiPDGraph` database, a graph database that contains datasets from various working groups. For a list of these compilations, see [this page](https://lipdverse.org/project/). \n",
    "\n",
    "In particular, we will be using PAGES2k and iso-2k for this demonstration.\n",
    "\n",
    "* PAGES2k: PAGES2k Consortium., Emile-Geay, J., McKay, N. et al. A global multiproxy database for temperature reconstructions of the Common Era. Sci Data 4, 170088 (2017). doi:10.1038/sdata.2017.88\n",
    "\n",
    "* iso2k: Konecky, B. L., McKay, N. P., Churakova (Sidorova), O. V., Comas-Bru, L., Dassié, E. P., DeLong, K. L., Falster, G. M., Fischer, M. J., Jones, M. D., Jonkers, L., Kaufman, D. S., Leduc, G., Managave, S. R., Martrat, B., Opel, T., Orsi, A. J., Partin, J. W., Sayani, H. R., Thomas, E. K., Thompson, D. M., Tyler, J. J., Abram, N. J., Atwood, A. R., Cartapanis, O., Conroy, J. L., Curran, M. A., Dee, S. G., Deininger, M., Divine, D. V., Kern, Z., Porter, T. J., Stevenson, S. L., von Gunten, L., and Iso2k Project Members: The Iso2k database: a global compilation of paleo-δ18O and δ2H records to aid understanding of Common Era climate, Earth Syst. Sci. Data, 12, 2261–2288, https://doi.org/10.5194/essd-12-2261-2020, 2020.\n",
    "\n",
    "## Demonstration\n",
    "\n",
    "Let's import a few web packages along with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50b75ae-b248-4dc4-a2a3-b888c1efbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7e8e0-f4ed-464d-8988-37330343bb13",
   "metadata": {},
   "source": [
    "### Simple query\n",
    "\n",
    "Let's start with one the simple queries we have been playing with. The `LiPDGraph` knowledge base is organized very similarly to a `LiPD` object, with each dataset stored in its own graph. So the queries you have been learning in the previous tutorials also apply. \n",
    "\n",
    "Let's retrieve 10 random dataset names. But first we need to point towards the endpoint for the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f391bd7-af5b-4c6f-8773-eb1cb8647df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://linkedearth.graphdb.mint.isi.edu/repositories/LiPDVerse-dynamic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3631f-9a52-4db6-b774-5687fee28e33",
   "metadata": {},
   "source": [
    "We periodocially version the LiPDGraph for reproducbility purposes. `LiPDverse-dynamic` refers to the most up-to-date versions of the LiPDGraph. Once your study is done, we encourage you to point towards the latest frozen version. \n",
    "\n",
    "Next, let's write our queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc33c2c1-9cb3-47fe-ad68-ac2e28bf4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "\n",
    "PREFIX le:<http://linked.earth/ontology#>\n",
    "\n",
    "SELECT ?ds ?dsname WHERE{\n",
    "\n",
    "?ds a le:Dataset .\n",
    "?ds le:hasName ?dsname .\n",
    "} \n",
    "\n",
    "ORDER BY RAND() LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579ce2c-03a6-4500-9175-0bfb3e5e8a7e",
   "metadata": {},
   "source": [
    "This should be very familiar as this is the first query we performed in the [custom queries tutorial](L2_custom_queries.md). Now, let's send it to our SPAQRL endpoint and retrieve the results that we will store in a pandas DataFrame.\n",
    "\n",
    "Note: The next cell might take a little bit of time to run since the query has to be performed on the remote server before sending the data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd38e03-793a-4dbf-96d1-a4e88d9f95ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>dsname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://linked.earth/lipd/SO201_2_101</td>\n",
       "      <td>SO201_2_101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://linked.earth/lipd/Craterlake.Arcusa.2020</td>\n",
       "      <td>Craterlake.Arcusa.2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://linked.earth/lipd/LlynCororion.Watkins....</td>\n",
       "      <td>LlynCororion.Watkins.2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://linked.earth/lipd/RosleNowe.Krajewski.1984</td>\n",
       "      <td>RosleNowe.Krajewski.1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://linked.earth/lipd/SS05ROES</td>\n",
       "      <td>SS05ROES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://linked.earth/lipd/LifebuoyLake.Soliviev...</td>\n",
       "      <td>LifebuoyLake.Solivieva.2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://linked.earth/lipd/AustreNevlingen.Kjell...</td>\n",
       "      <td>AustreNevlingen.Kjellman.2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://linked.earth/lipd/Cleveland.Davis.1986</td>\n",
       "      <td>Cleveland.Davis.1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://linked.earth/lipd/LI06FIJ01</td>\n",
       "      <td>LI06FIJ01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://linked.earth/lipd/SM06LKF02</td>\n",
       "      <td>SM06LKF02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ds  \\\n",
       "0               http://linked.earth/lipd/SO201_2_101   \n",
       "1    http://linked.earth/lipd/Craterlake.Arcusa.2020   \n",
       "2  http://linked.earth/lipd/LlynCororion.Watkins....   \n",
       "3  http://linked.earth/lipd/RosleNowe.Krajewski.1984   \n",
       "4                  http://linked.earth/lipd/SS05ROES   \n",
       "5  http://linked.earth/lipd/LifebuoyLake.Soliviev...   \n",
       "6  http://linked.earth/lipd/AustreNevlingen.Kjell...   \n",
       "7      http://linked.earth/lipd/Cleveland.Davis.1986   \n",
       "8                 http://linked.earth/lipd/LI06FIJ01   \n",
       "9                 http://linked.earth/lipd/SM06LKF02   \n",
       "\n",
       "                          dsname  \n",
       "0                    SO201_2_101  \n",
       "1         Craterlake.Arcusa.2020  \n",
       "2      LlynCororion.Watkins.2007  \n",
       "3       RosleNowe.Krajewski.1984  \n",
       "4                       SS05ROES  \n",
       "5    LifebuoyLake.Solivieva.2015  \n",
       "6  AustreNevlingen.Kjellman.2020  \n",
       "7           Cleveland.Davis.1986  \n",
       "8                      LI06FIJ01  \n",
       "9                      SM06LKF02  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(url, data = {'query': query})\n",
    "\n",
    "data = io.StringIO(response.text)\n",
    "df = pd.read_csv(data, sep=\",\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d710c4-66b5-42ed-be3d-aba38c42095b",
   "metadata": {},
   "source": [
    "### Complex queries\n",
    "\n",
    "Ok let's work on something more complex. Let's say that you want to retrieve all the data in the PAGES2k and iso2k compilations that were tagged to be used in the temperature analysis for this compilations along with metadata regarding the location, the type of archive and proxy used.\n",
    "\n",
    "Don't forget that each column is stored separately so you also need to go grab the time axis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7e8fb6-a51b-49eb-939c-edab47d434fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"PREFIX le: <http://linked.earth/ontology#>\n",
    "PREFIX le_var: <http://linked.earth/ontology/variables#>\n",
    "PREFIX wgs84: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "\n",
    "\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT DISTINCT ?dataSetName ?archiveType ?geo_meanLat ?geo_meanLon ?geo_meanElev \n",
    "    ?paleoData_variableName ?paleoData_values ?paleoData_units \n",
    "    ?paleoData_proxy ?paleoData_proxyGeneral ?time_variableName ?time_values \n",
    "\t?time_units ?compilationName ?TSID where{\n",
    "    \n",
    "    ?ds a le:Dataset .\n",
    "    ?ds le:hasName ?dataSetName .\n",
    "    OPTIONAL{\n",
    "            ?ds le:hasArchiveType ?archiveTypeObj .\n",
    "            ?archiveTypeObj rdfs:label ?archiveType .\n",
    "        }\n",
    "        \n",
    "    ?ds le:hasLocation ?loc .\n",
    "        OPTIONAL{?loc wgs84:lat ?geo_meanLat .}\n",
    "        OPTIONAL{?loc wgs84:long ?geo_meanLon .}      \n",
    "        OPTIONAL {?loc wgs84:alt ?geo_meanElev .}\n",
    "\n",
    "    ?ds le:hasPaleoData ?data .\n",
    "    ?data le:hasMeasurementTable ?table .\n",
    "    ?table le:hasVariable ?var .\n",
    "    ?var le:hasName ?paleoData_variableName .\n",
    "    ?var le:hasValues ?paleoData_values .\n",
    "    OPTIONAL{\n",
    "        ?var le:hasUnits ?paleoData_unitsObj .\n",
    "        ?paleoData_unitsObj rdfs:label ?paleoData_units .\n",
    "    }\n",
    "    OPTIONAL{\n",
    "        ?var le:hasProxy ?paleoData_proxyObj .\n",
    "        ?paleoData_proxyObj rdfs:label ?paleoData_proxy .\n",
    "    }\n",
    "    OPTIONAL{\n",
    "        ?var le:hasProxyGeneral ?paleoData_proxyGeneralObj .\n",
    "        ?paleoData_proxyGeneralObj rdfs:label ?paleoData_proxyGeneral .\n",
    "    }\n",
    "    ?var le:partOfCompilation ?compilation . \n",
    "    ?compilation le:hasName ?compilationName .\n",
    "    VALUES ?compilationName {\"iso2k\" \"Pages2kTemperature\"} .\n",
    "    ?var le:useInGlobalTemperatureAnalysis True .\n",
    "    OPTIONAL{\n",
    "        ?var le:hasVariableId ?TSID\n",
    "    } .\n",
    "        \n",
    "    ?table le:hasVariable ?timevar .\n",
    "    ?timevar le:hasName ?time_variableName .\n",
    "    ?timevar le:hasStandardVariable le_var:year .\n",
    "    ?timevar le:hasValues ?time_values .\n",
    "    OPTIONAL{\n",
    "        ?timevar le:hasUnits ?time_unitsObj .\n",
    "        ?time_unitsObj rdfs:label ?time_units .\n",
    "    }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5cda7-8c7b-403d-9186-44f49cc126b1",
   "metadata": {},
   "source": [
    "Pretty long! But let's take it paragraph by paragraph.\n",
    "\n",
    "* The first paragraph corresponds to our prefixes. You should be familiar with them by now.\n",
    "* The second paragraph lists all the variables that we wish to return:\n",
    "    * the name of the dataset\n",
    "    * the type of archive the measurements were made on\n",
    "    * geographical coordinates (lat/lon/elevation)\n",
    "    * the name of the paleovariable, associated units and values.\n",
    "    * The type of proxy used to obtain the data\n",
    "    * Information about the time axis (name, units and values)\n",
    "    * the name of the compilation the data is part of\n",
    "    * a unique identifer for the variables\n",
    "* The third paragraph:\n",
    "```\n",
    "?ds a le:Dataset .\n",
    "?ds le:hasName ?dataSetName .\n",
    "OPTIONAL{\n",
    "    ?ds le:hasArchiveType ?archiveTypeObj .\n",
    "    ?archiveTypeObj rdfs:label ?archiveType .\n",
    "        }\n",
    "```\n",
    "performs a query to return the information stored at the dataset level (namely the name of the dataset and the type of archive)\n",
    "* The fourth paragraph:\n",
    "```\n",
    "?ds le:hasLocation ?loc .\n",
    "OPTIONAL{?loc wgs84:lat ?geo_meanLat .}\n",
    "OPTIONAL{?loc wgs84:long ?geo_meanLon .}      \n",
    "OPTIONAL {?loc wgs84:alt ?geo_meanElev .}\n",
    "\n",
    "```\n",
    "returns geographical coordinates. \n",
    "* The fifth pragraph allows to dig to the variable level:\n",
    "```\n",
    "?ds le:hasPaleoData ?data .\n",
    "    ?data le:hasMeasurementTable ?table .\n",
    "    \n",
    "    ?table le:hasVariable ?var .\n",
    "    ?var le:hasName ?paleoData_variableName .\n",
    "    ?var le:hasValues ?paleoData_values .\n",
    "    \n",
    "    OPTIONAL{\n",
    "        ?var le:hasUnits ?paleoData_unitsObj .\n",
    "        ?paleoData_unitsObj rdfs:label ?paleoData_units .\n",
    "    }\n",
    "    OPTIONAL{\n",
    "        ?var le:hasProxy ?paleoData_proxyObj .\n",
    "        ?paleoData_proxyObj rdfs:label ?paleoData_proxy .\n",
    "    }\n",
    "    OPTIONAL{\n",
    "        ?var le:hasProxyGeneral ?paleoData_proxyGeneralObj .\n",
    "        ?paleoData_proxyGeneralObj rdfs:label ?paleoData_proxyGeneral .\n",
    "    }\n",
    "    ?var le:partOfCompilation ?compilation . \n",
    "    ?compilation le:hasName ?compilationName .\n",
    "    VALUES ?compilationName {\"iso2k\" \"Pages2kTemperature\"} .\n",
    "    ?var le:useInGlobalTemperatureAnalysis True .\n",
    "\n",
    "    OPTIONAL{\n",
    "        ?var le:hasVariableId ?TSID\n",
    "    } .\n",
    "```\n",
    "\n",
    "and asks for the name of the variable, its values, units, the proxy information, and unique identifier. Notice the two filters: one for the compilation the datasets belong to and the other to retain only the variables that these working groups have identified as being used in a global temperature analysis. \n",
    "\n",
    "* The last paragraph:\n",
    "```\n",
    "?timevar le:hasValues ?time_values .\n",
    "    OPTIONAL{\n",
    "        ?timevar le:hasUnits ?time_unitsObj .\n",
    "        ?time_unitsObj rdfs:label ?time_units .\n",
    "    }\n",
    "```\n",
    "looks for the time information associated with the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fb2fd-8f83-4584-8e4f-b0b4a032ea32",
   "metadata": {},
   "source": [
    "Let's send our query over to the server and get the results into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9875e5a6-b7d7-4678-9f33-293222806d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, data = {'query': query})\n",
    "\n",
    "data = io.StringIO(response.text)\n",
    "df = pd.read_csv(data, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2258eaf-629c-40c7-b311-0264a2a831a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataSetName</th>\n",
       "      <th>archiveType</th>\n",
       "      <th>geo_meanLat</th>\n",
       "      <th>geo_meanLon</th>\n",
       "      <th>geo_meanElev</th>\n",
       "      <th>paleoData_variableName</th>\n",
       "      <th>paleoData_values</th>\n",
       "      <th>paleoData_units</th>\n",
       "      <th>paleoData_proxy</th>\n",
       "      <th>paleoData_proxyGeneral</th>\n",
       "      <th>time_variableName</th>\n",
       "      <th>time_values</th>\n",
       "      <th>time_units</th>\n",
       "      <th>compilationName</th>\n",
       "      <th>TSID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LS16STCL</td>\n",
       "      <td>Lake sediment</td>\n",
       "      <td>50.8300</td>\n",
       "      <td>-116.3900</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>d18O</td>\n",
       "      <td>[-7.81, -5.91, -9.03, -5.35, -5.61, -5.98, -5....</td>\n",
       "      <td>permil</td>\n",
       "      <td>d18O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>year</td>\n",
       "      <td>[2009.0, 2008.3, 2007.8, 2007.4, 2007.0, 2006....</td>\n",
       "      <td>yr AD</td>\n",
       "      <td>iso2k</td>\n",
       "      <td>LPD7dc5b9ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO00URMA</td>\n",
       "      <td>Coral</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>173.0000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>d18O</td>\n",
       "      <td>[-4.8011, -4.725, -4.6994, -4.86, -5.0886, -5....</td>\n",
       "      <td>permil</td>\n",
       "      <td>d18O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>year</td>\n",
       "      <td>[1994.5, 1994.33, 1994.17, 1994.0, 1993.83, 19...</td>\n",
       "      <td>yr AD</td>\n",
       "      <td>iso2k</td>\n",
       "      <td>Ocean2kHR_177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CO05KUBE</td>\n",
       "      <td>Coral</td>\n",
       "      <td>32.4670</td>\n",
       "      <td>-64.7000</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>d18O</td>\n",
       "      <td>[-4.15, -3.66, -3.69, -4.07, -3.95, -4.12, -3....</td>\n",
       "      <td>permil</td>\n",
       "      <td>d18O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>year</td>\n",
       "      <td>[1983.21, 1983.13, 1983.04, 1982.96, 1982.88, ...</td>\n",
       "      <td>yr AD</td>\n",
       "      <td>iso2k</td>\n",
       "      <td>Ocean2kHR_105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IC13THQU</td>\n",
       "      <td>Glacier ice</td>\n",
       "      <td>-13.9333</td>\n",
       "      <td>-70.8333</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>d18O</td>\n",
       "      <td>[-18.5905, -16.3244, -16.2324, -17.0112, -18.6...</td>\n",
       "      <td>permil</td>\n",
       "      <td>d18O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>year</td>\n",
       "      <td>[2009, 2008, 2007, 2006, 2005, 2004, 2003, 200...</td>\n",
       "      <td>yr AD</td>\n",
       "      <td>iso2k</td>\n",
       "      <td>SAm_035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO01TUNG</td>\n",
       "      <td>Coral</td>\n",
       "      <td>-5.2170</td>\n",
       "      <td>145.8170</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>d18O</td>\n",
       "      <td>[-4.827, -4.786, -4.693, -4.852, -4.991, -4.90...</td>\n",
       "      <td>permil</td>\n",
       "      <td>d18O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>year</td>\n",
       "      <td>[1993.042, 1992.792, 1992.542, 1992.292, 1992....</td>\n",
       "      <td>yr AD</td>\n",
       "      <td>iso2k</td>\n",
       "      <td>Ocean2kHR_140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataSetName    archiveType  geo_meanLat  geo_meanLon  geo_meanElev  \\\n",
       "0    LS16STCL  Lake sediment      50.8300    -116.3900        1126.0   \n",
       "1    CO00URMA          Coral       0.9330     173.0000           6.0   \n",
       "2    CO05KUBE          Coral      32.4670     -64.7000         -12.0   \n",
       "3    IC13THQU    Glacier ice     -13.9333     -70.8333        5670.0   \n",
       "4    CO01TUNG          Coral      -5.2170     145.8170          -3.0   \n",
       "\n",
       "  paleoData_variableName                                   paleoData_values  \\\n",
       "0                   d18O  [-7.81, -5.91, -9.03, -5.35, -5.61, -5.98, -5....   \n",
       "1                   d18O  [-4.8011, -4.725, -4.6994, -4.86, -5.0886, -5....   \n",
       "2                   d18O  [-4.15, -3.66, -3.69, -4.07, -3.95, -4.12, -3....   \n",
       "3                   d18O  [-18.5905, -16.3244, -16.2324, -17.0112, -18.6...   \n",
       "4                   d18O  [-4.827, -4.786, -4.693, -4.852, -4.991, -4.90...   \n",
       "\n",
       "  paleoData_units paleoData_proxy  paleoData_proxyGeneral time_variableName  \\\n",
       "0          permil            d18O                     NaN              year   \n",
       "1          permil            d18O                     NaN              year   \n",
       "2          permil            d18O                     NaN              year   \n",
       "3          permil            d18O                     NaN              year   \n",
       "4          permil            d18O                     NaN              year   \n",
       "\n",
       "                                         time_values time_units  \\\n",
       "0  [2009.0, 2008.3, 2007.8, 2007.4, 2007.0, 2006....      yr AD   \n",
       "1  [1994.5, 1994.33, 1994.17, 1994.0, 1993.83, 19...      yr AD   \n",
       "2  [1983.21, 1983.13, 1983.04, 1982.96, 1982.88, ...      yr AD   \n",
       "3  [2009, 2008, 2007, 2006, 2005, 2004, 2003, 200...      yr AD   \n",
       "4  [1993.042, 1992.792, 1992.542, 1992.292, 1992....      yr AD   \n",
       "\n",
       "  compilationName           TSID  \n",
       "0           iso2k    LPD7dc5b9ba  \n",
       "1           iso2k  Ocean2kHR_177  \n",
       "2           iso2k  Ocean2kHR_105  \n",
       "3           iso2k        SAm_035  \n",
       "4           iso2k  Ocean2kHR_140  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358717fd-9e38-4f59-922e-ff7f09d35f89",
   "metadata": {},
   "source": [
    "Much like PyLiPD, the information is contained in columns with each row representing a time series. Let's have a look at the type in the `values` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f450153e-86d8-4a6b-b12e-5cae76ec1666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['paleoData_values'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11447a28-8e38-4e06-8ea8-698778e5f2ee",
   "metadata": {},
   "source": [
    "This list is actually a string. Why? Keep in mind that most knowledge bases won't contain the actual data, but rather a link to a file containing the data. So this is quite unique and an improvement. Could we have stored the data as lists of float? The problem comes down to memory and efficiency. So string was preferred.\n",
    "\n",
    "So what does that mean for your workflow? First, you will need to convert each row to numpy arrays or list. Here is a way to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201e39c5-b4d3-4adc-9b8b-feaf9a839407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['paleoData_values']=df['paleoData_values'].apply(lambda row : json.loads(row) if isinstance(row, str) else row)\n",
    "df['time_values']=df['time_values'].apply(lambda row : json.loads(row) if isinstance(row, str) else row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd07235f-60d6-42f5-9314-619d91e856a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['paleoData_values'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfc50b-2a33-424a-b82d-9f9d74842595",
   "metadata": {},
   "source": [
    "Second, it means that you cannot perform queries on the data themselves. We store some information about the data (e.g., maximum/minumum value, resolution) as metadata that is being sensed when a LiPD file is being created. So you actually perform queries on these to filter datasets. But you cannot query anything about the data directly (e.g., how many data points are in the timeseries, or between two time points in the series) until you have loaded them to your workspace. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
